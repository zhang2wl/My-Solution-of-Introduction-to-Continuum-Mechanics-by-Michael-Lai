{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 Part B: Tensors\n",
    "\n",
    "## Definition of Tensor\n",
    "\n",
    "Let $\\mathbf{T}$ be a transformation that transforms any vector into another vector. If $\\mathbf{T}$ transforms $\\mathbf{a}$ to $\\mathbf{b}$, and transforms $\\mathbf{c}$ to $\\mathbf{d}$, we write $\\mathbf{T}\\mathbf{a}=\\mathbf{b}$ and $\\mathbf{T}\\mathbf{c}=\\mathbf{d}$. If $\\mathbf{T}$ has the following properties:\n",
    "\\begin{equation}\n",
    "   \\mathbf{T}(\\mathbf{a}+\\mathbf{b}) = \\mathbf{T}\\mathbf{a} + \\mathbf{T}\\mathbf{b}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "   \\mathbf{T}(α\\mathbf{a}) = α\\mathbf{T}\\mathbf{a}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{a}$ and $\\mathbf{b}$ are arbitrary vectors and $\\alpha$ is a scalar, then we say $\\mathbf{T}$ is a _linear transformation_, or a _second-order tensor_, or simply _tensor_.\n",
    "\n",
    "If two tensors transform an arbitrary vector identically, then these two tensors are the same. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components of a Tensor\n",
    "\n",
    "The components of a tensor depends on the base vector used to describe the components. \n",
    "\n",
    "Let $\\mathbf{e}_1, \\mathbf{e}_2, \\mathbf{e}_3$ be base vectors of a coordinate system, and tensor $\\mathbf{T}$ transforms them into $\\mathbf{T}\\mathbf{e}_1, \\mathbf{T}\\mathbf{e}_2, \\mathbf{T}\\mathbf{e}_3$, each of them being a vector can be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "   \\mathbf{T} \\mathbf{e}_i = T_{ji} \\mathbf{e}_j\n",
    "\\end{equation}\n",
    "\n",
    "The components $T_{ji}$ are defined as the components of tensor $\\mathbf{T}$. They can be put in a matrix as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "   \\begin{bmatrix}\n",
    "      T_{11} & T_{12} & T_{13} \\\\\n",
    "      T_{21} & T_{22} & T_{23} \\\\\n",
    "      T_{31} & T_{32} & T_{33} \\\\\n",
    "   \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "This matrix is called _matrix of tensor_ $\\mathbf{T}$ with respect to the set of bases $\\mathbf{e}_i$.\n",
    "\n",
    "Multiply both sides of equation $\\mathbf{T} \\mathbf{e}_i = T_{ji} \\mathbf{e}_j$ by $\\mathbf{e}_m$, we get\n",
    "\n",
    "\\begin{equation}\n",
    "   \\mathbf{e}_m \\mathbf{T} \\mathbf{e}_i = \\mathbf{e}_m T_{ji} \\mathbf{e}_j = δ_{mj} T_{ji} = T_{mi}\n",
    "\\end{equation}\n",
    "\n",
    "Each of the component of $\\mathbf{T}$ can be expressed as:\n",
    "\\begin{equation}\n",
    "   T_{ij} = \\mathbf{e}_i \\cdot \\mathbf{T} \\mathbf{e}_j\n",
    "\\end{equation}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components of a Transformed Vector\n",
    "Given vector $\\mathbf{b}$ is transformed from $\\mathbf{a}$ by $\\mathbf{T}$, namely, $\\mathbf{b}=\\mathbf{T}\\mathbf{a}$, the components of $\\mathbf{b}$ can be expressed as:\n",
    "\n",
    "\\begin{equation}\n",
    "   b_{i} = T_{ij}a_j\n",
    "\\end{equation}\n",
    "\n",
    "This is the same form as the matrix equation $[\\mathbf{b}]=[\\mathbf{T}][\\mathbf{a}]$. To show that, we have\n",
    "\n",
    "\\begin{equation}\n",
    "   \\begin{aligned}\n",
    "      \\mathbf{b} &= b_{i} \\mathbf{e}_i \\\\ \n",
    "               & = \\mathbf{T} \\mathbf{a} \\\\\n",
    "               & = \\mathbf{T} (a_{j} \\mathbf{e}_j) \n",
    "   \\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\n",
    "      b_{i} = \\mathbf{e}_i \\mathbf{b} =  \\mathbf{e}_i \\mathbf{T} (a_{j} \\mathbf{e}_j) = a_{j} (\\mathbf{e}_i T \\mathbf{e}_j) = T_{ij} a_j\n",
    "\n",
    "\\end{equation}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum of Tensors\n",
    "Sum of tensors are consistent with the matrix form $W_{ij} = T_{ij} + S_{ij}$. Proof:\n",
    "\n",
    "The components of the sum is: $W_{ij} = \\mathbf{e}_i \\mathbf{(T+S)} \\mathbf{e}_j = T_{ij} + S_{ij}$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product of Tensors\n",
    "Using the definition of tensor, we can derive the product of a tensor:\n",
    "\n",
    "$\\mathbf{W}=\\mathbf{T}\\mathbf{S}$ →\n",
    "$W_{ij} = \\mathbf{e}_i (\\mathbf{TS}) \\mathbf{e}_j\n",
    "        = \\mathbf{e}_i \\mathbf{T} (\\mathbf{S}\\mathbf{e}_j)\n",
    "        = \\mathbf{e}_i \\mathbf{T} S_{mj} \\mathbf{e}_m\n",
    "        = T_{in}\\mathbf{e}_n S_{mj} \\mathbf{e}_m\n",
    "        = T_{in} S_{mj} δ_{mn}\n",
    "        = T_{im} S_{mj}$\n",
    "\n",
    "Similarly, $\\mathbf{W}=\\mathbf{S}\\mathbf{T}$ → $W_{ij} = S_{im} T_{mj}$\n",
    "\n",
    "It is clear that $\\mathbf{S}\\mathbf{T} ≠ \\mathbf{T}\\mathbf{S}$. That is, in general, tensor product is not commutative. But with the tensor nature, it can be easily seen that $\\mathbf{S}\\mathbf{T}\\mathbf{a}=\\mathbf{S}(\\mathbf{T}\\mathbf{a})$. Thus, tensor product is associative. And it is natural to define $\\mathbf{T}\\mathbf{T}=\\mathbf{T}^2$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transpose of a tensor\n",
    "\n",
    "The transpose of a tensor $\\mathbf{T}$, by definition, satisfies $\\mathbf{a}\\mathbf{T}\\mathbf{b} = \\mathbf{b} \\mathbf{T}^T \\mathbf{a}$. It can be easily seen that $T_{ij} = T^T_{ji}$ as $\\mathbf{e}_i ⋅\\mathbf{T} \\mathbf{e}_j = \\mathbf{e}_j ⋅\\mathbf{T}^T \\mathbf{e}_i$. \n",
    "\n",
    "It can also be proved that $(\\mathbf{T}^T)^T = \\mathbf{T}$ as $\\mathbf{a}\\mathbf{T}^T\\mathbf{b} = \\mathbf{b} (\\mathbf{T}^T)^T \\mathbf{a}$\n",
    "\n",
    "It can also be easily established that $(\\mathbf{T}\\mathbf{S})^T = \\mathbf{S}\\mathbf{T}$ as $(\\mathbf{T}\\mathbf{S})^T = W_{ij}^T = W_{ji} = \\mathbf{S}\\mathbf{T}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dyadic product of vectors\n",
    "\n",
    "The dyadic product of vectors $\\mathbf{a}$ and $\\mathbf{b}$, denoted by $\\mathbf{a}\\mathbf{b}$ or $\\mathbf{a}⊗\\mathbf{b}$, is defined to be the transformation that transforms any vector $\\mathbf{c}$ according to the rule:\n",
    "\n",
    "$\\mathbf{a}⊗\\mathbf{b}\\mathbf{c} = \\mathbf{a}(\\mathbf{b}⋅\\mathbf{c})$\n",
    "\n",
    "Now, for any vectors $\\mathbf{c}$ and $\\mathbf{d}$, and any scalars $α$ and $β$, we have\n",
    "\n",
    "\\begin{aligned}\n",
    "(\\mathbf{a}⊗\\mathbf{b})(α\\mathbf{c} + β\\mathbf{d}) &= \\mathbf{a}(\\mathbf{b}⋅(α\\mathbf{c}+β\\mathbf{d})) \\\\\n",
    "&= \\mathbf{a}(\\mathbf{b}⋅α\\mathbf{c}+\\mathbf{b}⋅β\\mathbf{d})) \\\\\n",
    "&= α\\mathbf{a}(\\mathbf{b}⋅\\mathbf{c})+β\\mathbf{a}(\\mathbf{b}⋅\\mathbf{d}) \\\\\n",
    "&= α(\\mathbf{a}⊗\\mathbf{b})\\mathbf{c} + β(\\mathbf{a}⊗\\mathbf{b})\\mathbf{d}\n",
    "\\end{aligned}\n",
    "\n",
    "Thus the dyadic product $\\mathbf{a}⊗\\mathbf{b}$ is a linear transformation.\n",
    "\n",
    "Let $\\mathbf{W} = \\mathbf{a}⊗\\mathbf{b}$, then the components of $\\mathbf{W}$ are:\n",
    "$W_{ij} = \\mathbf{e}_i ⋅ \\mathbf{W} \\mathbf{e}_j = \\mathbf{e}_i ⋅ \\mathbf{a}⊗\\mathbf{b} \\mathbf{e}_j\n",
    "= \\mathbf{e}_i ⋅ (\\mathbf{a}⊗\\mathbf{b} \\mathbf{e}_j)\n",
    "= \\mathbf{e}_i ⋅ (\\mathbf{a} (\\mathbf{b} \\mathbf{e}_j))\n",
    "= \\mathbf{e}_i ⋅ (\\mathbf{a} b_j) = a_i b_j $\n",
    "\n",
    "It is clear that any tensor $\\mathbf{T}$ can be expressed as $\\mathbf{T} = T_{ij}\\mathbf{e}_i\\mathbf{e}_j$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace of a Tensor\n",
    "\n",
    "The trace of a tensor is a scalar the obeys the following rules: For any tensor $\\mathbf{T}$ and $\\mathbf{S}$ and any vector $\\mathbf{a}$ and $\\mathbf{b}$\n",
    "\n",
    "\\begin{aligned}\n",
    "   tr(\\mathbf{T}+\\mathbf{S}) & = tr\\mathbf{T} + tr\\mathbf{S} \\\\\n",
    "   tr(α\\mathbf{T}) & = αtr\\mathbf{T} \\\\\n",
    "   tr(\\mathbf{a}⊗\\mathbf{b}) &= \\mathbf{a} ⋅ \\mathbf{b}\n",
    "\\end{aligned}\n",
    "\n",
    "It can be easily proved that $tr\\mathbf{T}\\mathbf{S} = tr(\\mathbf{S}\\mathbf{T})$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity tensor and tensor inverse\n",
    "\n",
    "The linear transformation that transforms every vector to itself is called an _identity tensor_. Denoting this special tensor by $\\mathbf{I}$, we have for any $\\mathbf{a}$,\n",
    "\n",
    "$\\mathbf{I}\\mathbf{a}=\\mathbf{a}$\n",
    "\n",
    "$I_{ij} = δ_{ij}$\n",
    "\n",
    "Given a tensor $\\mathbf{T}$, if a tensor $\\mathbf{S}$ exists such that $\\mathbf{T}\\mathbf{S} = \\mathbf{I}$, then we say $\\mathbf{S}$ is the inverse of $\\mathbf{T}$. In order to find the inverse of $\\mathbf{T}$, its determinant cannot be 0, that is $\\mathbf{T}$ cannot be singular. \n",
    "\n",
    "It can be shown that for the tensor inverse, the following relations are satisfied:\n",
    "\n",
    "$(\\mathbf{T}^T)^{-1} = (\\mathbf{T}^{-1})^{T}$\n",
    "\n",
    "Proof:\n",
    "\n",
    "$\\mathbf{T} \\mathbf{T}^{-1} = \\mathbf{T}^{-1} \\mathbf{T} = \\mathbf{I}$\n",
    "\n",
    "$(\\mathbf{T} \\mathbf{T}^{-1})^T = (\\mathbf{T}^{-1})^T \\mathbf{T}^T = \\mathbf{I} = (\\mathbf{T}^T)^{-1}\\mathbf{T}^T$\n",
    "\n",
    "Thus \n",
    "\n",
    "$(\\mathbf{T}^T)^{-1} = (\\mathbf{T}^{-1})^{T}$\n",
    "\n",
    "\n",
    "It can also be shown that $(\\mathbf{T}\\mathbf{S})^{-1} = \\mathbf{S}^{-1}\\mathbf{T}^{-1}$:\n",
    "\n",
    "$(\\mathbf{T}\\mathbf{S})(\\mathbf{S}^{-1}\\mathbf{T}^{-1}) = \n",
    "\\mathbf{T}(\\mathbf{S})(\\mathbf{S}^{-1})\\mathbf{T}^{-1} = \n",
    "\\mathbf{T}\\mathbf{T}^{-1} = \\mathbf{I} $\n",
    "\n",
    "Thus $\\mathbf{S}^{-1}\\mathbf{T}^{-1} = (\\mathbf{T}\\mathbf{S})^{-1}$\n",
    "\n",
    "If the inverse of a tensor $\\mathbf{T}$ exists, then from $\\mathbf{T}\\mathbf{a}=\\mathbf{b}$ → $\\mathbf{a}=\\mathbf{T}^{-1}\\mathbf{b}$, means when a tensor is inversible, then there is one-to-one mapping of two vectors. On the other hand, if a tensor is not inversible, then there are more than one vector $\\mathbf{a}$ that can transform into $\\mathbf{b}$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonal tensors\n",
    "\n",
    "An _orthogonal tensor_ is a linear transformation under which transformed vectors preserve their lengths and angles. Let $\\mathbf{Q}$ be an orthonormal tensor, the by definition, $|\\mathbf{Q}\\mathbf{a}| = |\\mathbf{a}|$ and $cos(\\mathbf{Q}\\mathbf{a}, \\mathbf{Q}\\mathbf{b}) = cos(\\mathbf{a}, \\mathbf{b})$. Therefore\n",
    "\n",
    "\\begin{equation}\n",
    "   \\mathbf{Q}\\mathbf{a} ⋅ \\mathbf{Q}\\mathbf{b} = \\mathbf{a} ⋅ \\mathbf{b}\n",
    "\\end{equation}\n",
    "\n",
    "for any vectors\n",
    "\n",
    "Since by definition of transpose, $\\mathbf{a}⋅\\mathbf{b} = (\\mathbf{Q}\\mathbf{a})⋅(\\mathbf{Q}\\mathbf{b}) = \\mathbf{b} ⋅ \\mathbf{Q}^T (\\mathbf{Q}\\mathbf{a})$, thus\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "   \\mathbf{Q}\\mathbf{Q}^T = \\mathbf{I}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This means for an orthogonal tensor, its inverse is simply its transpose. In tensor notation, we have \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "   Q_{mi} Q_{mj} = δ_{ij}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "<div class=\"notebox\">\n",
    "    <p>An example of orthogonal tensor is reflection matrix.</p>\n",
    "    <p>Another example is rigid body rotation.</p>\n",
    "</div>\n",
    "\n",
    "<style>\n",
    ".notebox {\n",
    "    background-color: lightgray;\n",
    "    border: 1px solid gray;\n",
    "    border-radius: 5px;\n",
    "    padding: 10px;\n",
    "    margin: 10px;\n",
    "    max-width: 100%;\n",
    "    width: auto;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation matrix between two rectangular systems\n",
    "\n",
    "Suppose that ${\\mathbf{e}_1,\\mathbf{e}_2,\\mathbf{e}_3}$ and ${\\mathbf{e}_1',\\mathbf{e}_2',\\mathbf{e}_3'}$ are the unit vectors corresponding to two rectangular Cartesian coordinate systems. It is clear that they can be made coincident to each other through transformation. Let $\\mathbf{Q}$ be the tensor to transform between them.\n",
    "\n",
    "\\begin{aligned}\n",
    "   \\mathbf{e}_i' &= \\mathbf{Q} \\mathbf{e}_i \\\\\n",
    "   \\mathbf{e}_i' &= Q_{mi} \\mathbf{e}_m\n",
    "\\end{aligned}\n",
    "\n",
    "where $Q_{mi}Q_{mj} = δ_{ij}$ or $\\mathbf{Q}^T\\mathbf{Q} = \\mathbf{I}$\n",
    "\n",
    "Note that $Q_{11} = \\mathbf{e}_1 ⋅ \\mathbf{Q}\\mathbf{e}_1 = \\mathbf{e}_1 ⋅ \\mathbf{e}_1'$ is the cosine of angle between $\\mathbf{e}_1$ and $\\mathbf{e}_1'$. More generally, $Q_{ij} = \\mathbf{e}_i ⋅ \\mathbf{Q}\\mathbf{e}_j = \\mathbf{e}_i ⋅ \\mathbf{e}_j'$, we may also write:\n",
    "\n",
    "$Q_{ij} = cos(\\mathbf{e}_i, \\mathbf{e}_j')$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation law for Cartesion components of a vector\n",
    "\n",
    "Consider any vector $\\mathbf{a}$, the components of it with respect to bases ${\\mathbf{e}_1,\\mathbf{e}_2,\\mathbf{e}_3}$ are:\n",
    "\n",
    "\\begin{equation}\n",
    "   a_i = \\mathbf{a}⋅\\mathbf{e}_i\n",
    "\\end{equation}\n",
    "\n",
    "and its components with respect to ${\\mathbf{e}_1',\\mathbf{e}_2',\\mathbf{e}_3'}$ are:\n",
    "\n",
    "\\begin{equation}\n",
    "   a_i' = \\mathbf{a}⋅\\mathbf{e}_i'\n",
    "\\end{equation}\n",
    "\n",
    "We have the relationship $\\mathbf{e}_i' = Q_{mi} \\mathbf{e}_m$, therefore:\n",
    "\n",
    "\\begin{equation}\n",
    "   a_i' = \\mathbf{a}⋅(Q_{mi} \\mathbf{e}_m) = Q_{mi} \\mathbf{a}⋅\\mathbf{e}_m = Q_{mi} a_m\n",
    "\\end{equation}\n",
    "\n",
    "In matrix form, we have:\n",
    "\n",
    "\\begin{equation}\n",
    "[\\mathbf{a}]' = [\\mathbf{Q}]^T [\\mathbf{a}]\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mathbf{a}' = \\begin{bmatrix}\n",
    "a_1' \\\\\n",
    "a_2' \\\\\n",
    "a_3' \\\\\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "\\mathbf{Q} = \\begin{bmatrix}\n",
    "Q_{11} & Q_{12} & Q_{13} \\\\\n",
    "Q_{21} & Q_{22} & Q_{23} \\\\\n",
    "Q_{31} & Q_{32} & Q_{33} \\\\\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "\\mathbf{a} = \\begin{bmatrix}\n",
    "a_1 \\\\\n",
    "a_2 \\\\\n",
    "a_3 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This is the transformation law relating components of the _same_ vector with respect to different Cartesian unit bases. It is very important to note this is not the same as $\\mathbf{a}' = \\mathbf{Q}^T \\mathbf{a}$. $[\\mathbf{a}]'$ and $[\\mathbf{a}]$ are the same vector with respect to different unit bases, while $\\mathbf{a}'$ and $\\mathbf{a}$ are different vectors.\n",
    "\n",
    "Let's premultiply by $[\\mathbf{Q}]$, we get:\n",
    "\n",
    "\\begin{equation}\n",
    "   [\\mathbf{a}] = [\\mathbf{Q}][\\mathbf{a}]'\n",
    "\\end{equation}\n",
    "\n",
    "The indicial notation is:\n",
    "\n",
    "\\begin{equation}\n",
    "   a_i = Q_{im} a'_m\n",
    "\\end{equation}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
